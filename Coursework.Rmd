---
title: "Coursera Practical Machine Learning with R Coursework"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

First we need to load the various libraries we will need for this assignment as well as to load in the training and test data from csv

```{r load}
library(caret)
library(e1071)
library(rattle)
library(rpart)
# When reading in the training data, we want to replace empty strings with NA so that we can treat empty strings and NA the same way in the preprocessing steps
training.raw <- read.csv("pml-training.csv", na.strings = c("", "NA"))
testing.raw <- read.csv("pml-testing.csv")
```

Now that we have loaded the data, we need to perform some preprocessing steps
1. First we will remove columns 1 to 6 as these contain values that would not make sense as predictors such as row id, username and timestamps
2. We will also remove any columns containing NA. We could have used techniques like kNN impute to fill missing values, but after some experimentation, eliminating all the columns with NA values does not seem to stop us from being able to create a very good prediction model for "classe". Removing columns containing NA brings us down to 55 columns
3. Principal component analysis PCA: trying to train models without first applying PCA was causing performance issues on my laptop, so I apply PCA to capture 95% of the variance and the performance improvements in model training were significant - from an hour+ to several minutes. PCA resulted in less than 50% nbr of features required

Once we have done this, we will split the training data into 80/20 split of training and testing data, since we should only RUN our final model on the holdout set (20 rows) without exploring the data and without training our hyperparameters on it

```{r preprocess/clean up the data}
# Remove columns 1-6 which do not make sensible predictors in training set
training.first6columnsremoved <- training.raw[ -c(1:6) ]
# Remove columns containing NA in training set
training.first6columnsandnasremoved <- training.first6columnsremoved[, which(colMeans(!is.na(training.first6columnsremoved)) == 1)]
# Remove columns 1-6 which do not make sensible predictors in testing set
testing.first6columnsremoved <- testing.raw[ -c(1:6) ]
# Remove columns containing NA in testing set
testing.first6columnsandnasremoved <- testing.first6columnsremoved[, which(colMeans(!is.na(training.first6columnsremoved)) == 1)]
inTrain = createDataPartition(y = training.first6columnsandnasremoved$classe, p = 0.8, list=FALSE)
training = training.first6columnsandnasremoved[ inTrain,]
testing = training.first6columnsandnasremoved[-inTrain,]
set.seed(123)
#calculate the # of columns in the dataset, as we know that the last column is the target variable "classe"
col_classe_nb<-dim(training)[2] 
#preprocess the data in order to get 95% of the variance
preProc <- preProcess(training[,-col_classe_nb],method="pca",thresh=0.95) 
#Apply preProc object to training and testing and holdout dataset
training<-predict(preProc,training)
testing<-predict(preProc,testing)
holdout<-predict(preProc,testing.first6columnsandnasremoved)
preProc
```

First we will try a decision tree model using rpart and we can plot it using rattle  

```{r tree}
set.seed(123)
fitRpart <- rpart(classe ~ ., data=training, method="class")
fancyRpartPlot(fitRpart, caption = "Visual representation of tree to predict classe")
```

Now we will estimate the out of sample accuracy for the tree model

```{r tree accuracy}
pred.rpart <- predict(fitRpart, testing, type="class")
acc.rpart <- confusionMatrix(table(factor(pred.rpart), factor(testing$classe)))$overall[1]
print(paste0("Out of sample estimated accuracy on rpart is ", acc.rpart))
```

Next we will try a random forest model using 5 fold cross-validation, which based on having seen data scientists that I work with using AutoML, usually seems to come out near or at the top of the model scoring leaderboard (along with gradient boosted trees)
I chose 75 trees based on experimentation and a compromise between model accuracy and training time needed

```{r random forest}
set.seed(123)
train.control <- trainControl(method = "cv", number = 5)
fitRf <- train(classe ~ ., data=training, method="rf", ntree=75, trControl=train.control)
pred.rf <- predict(fitRf, testing)
acc.rf <- confusionMatrix(table(factor(pred.rf), factor(testing$classe)))$overall[1]
print(paste0("Out of sample estimated accuracy on rf is ", acc.rf))
```

Next we will try a gradient boosted model using 5 fold cross-validation, which based on having seen data scientists that I work with using AutoML, usually seems to come out near or at the top of the model scoring leaderboard (along with random forest)

```{r gradient boosted model}
set.seed(123)
fitGbm <- train(classe ~ ., data=training, method="gbm", trControl=train.control, verbose = FALSE)
pred.gbm <- predict(fitGbm, testing)
acc.gbm <- confusionMatrix(table(factor(pred.gbm), factor(testing$classe)))$overall[1]
print(paste0("Out of sample estimated accuracy on gbm is ", acc.gbm))
```

The highest performing model based on experimentation was the random forest model (97%+ accuracy), so we will choose this as our final model and apply it on the 20 rows that we have in the holdout set.

```{r final model and predictions on holdout set}
finalpredictions <- predict(fitRf, holdout)
print(finalpredictions)
```